# Solved: More randomness is needed in mcts for nn searchs. Training data
# converges to some types of play.

#TODO: Use convolution, alphazero uses convolution and residual connections.

# Solved: It plays the same game over and over on low temperatures

# NOTE: Cross entropy seems to be negative because I show the difference
# between the best possible cross entropy: entropy of data distribution.
# Batch losses may end up smaller than that.

# Solved: eval_loss cannot be overfit to 50 training examples
# generated by a a network self-play. 0.2-0.3 training error remains.
# How: In TicTacToe, most data have the same input but different
# outcomes. Since the search is more or less the same with random network
# dist outputs are similar. However, the game ends with different results
# so evaluation loss cannot be made smaller due to the contradictory
# training data. It is not a function.

# Solved: Translator is broken! The network does not receive whose turn
# it is. I realized this when I saw that move error decreased but evalaution
# error did not. We want the evaluation wrt white it's why it couldn't
# learn the data. We should either want the evalaution wrt the player
# with the turn, and rotate the board accordingly, or we should 
# clearly make the players accessible to the network. #FIXED
# But why it didn't learn anything? Even though it is unclear whose move
# is that, it still knows the white and black pieces. Even more interestingly
# it learned the move distribution without knowing whose move it is.
# Even if we supplied it lacking information, it should have memorized
# the data! It did not even memorize it!

# Potential #BUG
# Does it take the winner moves or all moves for training?

# Potential #BUG
# Reversi translator translates the winning chances of white.
# In Alphazero the board was given from the playing person's perspective
# and evaluation was given wrt the player.

# Potential #BUG: Should we include only a fraction of the game states?

# Note: We should not give correlated training examples because it
# alters the settings SGD is used in.

# Potential Improvement Idea:
    # Make observable parameters some objects having the same type. By making
    # such a versatile and high entropy class, tracking parameters may
    # become easier. Or it may become worse due to the need to conform
    # to an interface.
    
# IDEA #TODO: In AlphaGo paper, they may be holding-out some data to check whether
# the network overfits to the examples given to it. They say their network
# overfitted due to "strongly corrolated" training examples. Their overfit
# was in a supervised KGS dataset, but the same may hold for self-play.
# They solved this issue by picking a single position from 30 million self-
# play games. I wish I had such compute power...



#%%

import pprint
from time import sleep
import itertools
import math
import random
import importlib
import matplotlib.pyplot as plt
import pickle

import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.nn.functional import mse_loss

from game.tictactoe import TicTacToe as TCT
from game.reversi import Reversi
from mcts import mcts, uct
from mcts.move_selectors import alpha_zero_move_selector_factory as azms
from mcts.move_selectors import UCT_move_selector_factory
from evaluator.evaluators import random_playout_evaluator
from data_generator import generate_data
from network_translator import (StandardTicTacToeTranslator,
                                StandardReversiTranslator)
# from network_translator import (StandardTicTacToeTranslator,
#                                 StandardReversiTranslator)
from neural_network import ResidualBlock, WithResidual
from game_session import ConsoleGameSession
from agent.agents import (MCTSAgent, user_for_game,
                          RandomAgent, UCTAgent, EvaluatorAgent)
from training_tools import compare
from miscellaneous import cross_entropy


def load_all_uct_data_TCT():
    
    if 'uct_data' in globals():
        raise RuntimeError('It already exists.')
    
    with open('tct_all_UCT_data.pickle','rb') as file:
        global uct_data
        uct_data = pickle.load(file)
        random.shuffle(uct_data)


def discretize_evaluation_TCT(evaluation):
    
    x_bets = evaluation[TCT.X]
    
    if x_bets > 0.5:
        pred_result = 1
    elif x_bets < -0.5:
        pred_result = -1
    else:
        pred_result = 0
    
    return pred_result


def default_network_for_game(game, device):
    # Networks should have their own translators.
    if game == TCT:
        return TCTNetwork(device)
    elif game == Reversi:
        return ReversiNetwork(device)
    else:
        raise NotImplementedError()


def get_evaluator(network, device):
    def evaluator(state, legal_moves = None, player = None):
        translator = network.translator
        
        if legal_moves == None:
            legal_moves = state.moves()
        if player == None:
            player = state.turn()
            
        x = translator.states_to_tensor([state], device)
        move_dist, evaluation = network(x)
        return (
            translator.tensor_to_dists([state], [legal_moves], move_dist)[0], 
            translator.tensor_to_evals([state], evaluation)[0])
    return evaluator


class TCTNetwork(nn.Module):

    def __init__(self, device):
        super(TCTNetwork, self).__init__()

        self.translator = StandardTicTacToeTranslator()

        self.body = nn.Sequential(
            # nn.Linear(27, 1000, device=device),
            # nn.ReLU(),
            # ResidualBlock(1000, device=device),
            # ResidualBlock(1000, device=device),
            # ResidualBlock(1000, device=device),
            # ResidualBlock(1000, device=device),
            # ResidualBlock(1000, device=device),
            # ResidualBlock(1000, device=device),
            
            # nn.Linear(27, 1000, device=device),
            # nn.ReLU(),
            
            nn.Linear(27, 1000, device=device),
            nn.ReLU(),
            WithResidual(nn.Sequential(
                nn.Linear(1000,1000, device=device),
                nn.ReLU(),
                nn.Linear(1000,1000, device=device),
                )),
            nn.ReLU(),
            
            WithResidual(nn.Sequential(
                nn.Linear(1000,1000, device=device),
                nn.ReLU(),
                nn.Linear(1000,1000, device=device),
                )),
            nn.ReLU(),
            
            WithResidual(nn.Sequential(
                nn.Linear(1000,1000, device=device),
                nn.ReLU(),
                nn.Linear(1000,1000, device=device),
                )),
            nn.ReLU(),
            
            WithResidual(nn.Sequential(
                nn.Linear(1000,1000, device=device),
                nn.ReLU(),
                nn.Linear(1000,1000, device=device),
                )),
            nn.ReLU(),
            
            
            )

        self.dist_head = nn.Sequential(
        
            # nn.Linear(1000,1000, device=device),
            # nn.ReLU(),
            # nn.Linear(1000,1000, device=device),
            # nn.ReLU(),
            # nn.Linear(1000,1000, device=device),
            # nn.ReLU(),
            # nn.Linear(1000,1000, device=device),
            # nn.ReLU(),
            # nn.Linear(1000,1000, device=device),
            # nn.ReLU(),
            # nn.Linear(1000,1000, device=device),
            # nn.ReLU(),
            # nn.Linear(1000,1000, device=device),
            # nn.ReLU(),
            
            
            
            nn.Linear(1000, 9, device=device),
            # nn.Linear(1000, 9, device=device),
            nn.Softmax(dim=1)
            )

        self.eval_head = nn.Sequential(
            # nn.Linear(1000,1000, device=device),
            # nn.ReLU(),
            # nn.Linear(1000,1000, device=device),
            # nn.ReLU(),
            # nn.Linear(1000,1000, device=device),
            # nn.ReLU(),
            # nn.Linear(1000,1000, device=device),
            # nn.ReLU(),
            # nn.Linear(1000,1000, device=device),
            # nn.ReLU(),
            
            
            nn.Linear(1000, 1, device=device),
            # nn.Linear(1000, 1, device=device),
            nn.Tanh()
            )

    def forward(self, x):
        x = self.body(x)
        return self.dist_head(x), self.eval_head(x)

class ReversiNetwork(nn.Module):
    
    def __init__(self, device):
        super(ReversiNetwork, self).__init__()
        
        self.translator = StandardReversiTranslator()
        
        self.body = nn.Sequential(
            nn.Linear(3*8*8, 100),
            nn.ReLU(),
            
            )
    
        self.dist_head = nn.Sequential(
            nn.Linear(100, 8*8 + 1, device=device),
            nn.Softmax(dim=1),
            )
        
        self.eval_head = nn.Sequential(
            nn.Linear(100, 1, device=device),
            nn.Tanh(),
            )
        
    def forward(self, x):
        x = self.body(x)
        return self.dist_head(x), self.eval_head(x)

class Training_Environment:
    
    def __init__(self,
                game,
                net,
                loop_count,
                mpdev,
                trdev,
                d_loss,
                e_loss,
                mcts_step_self_play,
                move_selector_self_play,
                temperature_self_play,
                lr,
                momentum,
                bs,
                mcts_step_testing,
                move_selector_testing,
                temperature_testing,
                max_data_length,
                batches,
                new_states,
                dirichlet_noise_parameter,
                noise_contribution,
                loss_record_frequency,
                initial_pool,
                ):
        
        
        self.game = game
        self.net = net
        self.loop_count = loop_count
        self.mpdev = mpdev
        self.trdev = trdev
        self.d_loss = d_loss
        self.e_loss = e_loss
        self.mcts_step_self_play = mcts_step_self_play
        self.move_selector_self_play = move_selector_self_play
        self.temperature_self_play = temperature_self_play
        self.lr = lr
        self.momentum = momentum
        self.bs = bs
        self.mcts_step_testing = mcts_step_testing
        self.move_selector_testing = move_selector_testing
        self.temperature_testing = temperature_testing
        self.max_data_length = max_data_length
        self.batches = batches
        self.new_states = new_states
        self.dirichlet_noise_parameter = dirichlet_noise_parameter
        self.noise_contribution = noise_contribution
        self.loss_record_frequency = loss_record_frequency
        self.initial_pool = initial_pool
        
        self.raw_pool = []
        
        self.random_win_rates = []
        self.dist_evaluation_random_win_rates =[]
        self.uct_100_win_rates = []
        self.uct_1000_win_rates = []
        self.data_test = []
        self.pool_length_record = []
        self.uct100_vs_random_win_rates = []
        self.eval_evaluation_random_win_rates = []
        
        self.dist_losses_before_training = []
        self.eval_losses_before_training = []
        
        self.dist_losses_after_training = []
        self.eval_losses_after_training = []
        
    def test_in(self):
        # Create agents from the network with the testing parameters
        print("Testing is starting... ")
        self.net.eval()
        self.net.to(self.mpdev)
        
        raw_ev = get_evaluator(self.net, self.mpdev)
        
        dist_evaluator_agent = EvaluatorAgent(raw_ev,
                   distribution_weight=1.,
                   evaluation_weight=0.)
        
        eval_evaluator_agent = EvaluatorAgent(raw_ev,
                                              distribution_weight=0.,
                                              evaluation_weight=1.)
        
        mcts_agent = MCTSAgent(raw_ev,
                          self.mcts_step_testing,
                          self.move_selector_testing,
                          self.temperature_testing)
        
        self.random_win_rates.append(
            compare(self.game,
                    [mcts_agent, RandomAgent()], 200,
                    normalize=True)[mcts_agent])
        
        self.dist_evaluation_random_win_rates.append(
            compare(self.game,
                    [dist_evaluator_agent, RandomAgent()], 200,
                    normalize=True)[dist_evaluator_agent])
        
        self.eval_evaluation_random_win_rates.append(
            compare(self.game,
                    [eval_evaluator_agent, RandomAgent()], 200,
                    normalize=True)[eval_evaluator_agent])
    
    def training(self):
        # TODO: record the environment together with the network 
        # together with the loss graph into a file at given time
        # intervals.
        
        print('Training is starting...\n')
        
        
        # Test first to see the initial performance
        self.net.eval()
        self.net.to(device=self.mpdev)
        
        self.test_in()
        
        # First values of auxiliaries
        
        self.pool_length_record.append(0)
        
        # uct100 = UCTAgent(100)
        # self.uct100_vs_random_win_rates.append(
        #     compare(self.game,
        #             [uct100, RandomAgent()], 1,
        #             normalize=True)[uct100])
        
        for i in range(1, 1 + self.loop_count):
            print("\nLoop number {}/{} is starting...\n".format(
                                                        i, self.loop_count))
            
            # Add to the pool via self-play
            self.net.to(self.mpdev)
            self.net.eval()
            
            game_states_to_be_added = (
                self.new_states if hasattr(self, 'x_pool')
                else self.initial_pool)
            
            self.generate_games(game_states_to_be_added,
                                ret=None, append_to_pool=True)
            
            #Measure the losses before training
            self.net.to(device=self.trdev)
            self.net.eval()
            
            dist_loss, eval_loss = self.get_losses()
            self.dist_losses_before_training.append(dist_loss)
            self.eval_losses_before_training.append(eval_loss)
            del dist_loss, eval_loss # Prevent bugs
            
            
            # Backprop training
            
            self.net.to(device=self.trdev)
            self.net.train()
            
            train_data(self.net, self.x_pool, self.dist_pool, self.eval_pool,
                       self.lr, self.momentum, self.bs,
                       self.batches, self.trdev, self.d_loss, self.e_loss,
                       graph = True,
                       record_frequency=self.loss_record_frequency)
            
            #Measure the losses after training
            self.net.eval()
            self.net.to(device=self.trdev)
            
            dist_loss, eval_loss = self.get_losses()
            self.dist_losses_after_training.append(dist_loss)
            self.eval_losses_after_training.append(eval_loss)
            del dist_loss, eval_loss # Prevent bugs
            
            # Test
            self.net.eval()
            self.net.to(device=self.mpdev)
            
            self.test_in()
            
            # Keep track of auxiliary variables
            
            self.pool_length_record.append(
                                self.x_pool.shape[0]/self.max_data_length)
            
            # uct100 = UCTAgent(100)
            # self.uct100_vs_random_win_rates.append(
            #     compare(self.game,
            #             [uct100, RandomAgent()], 1,
            #             normalize=True)[uct100])
            
            #Visualization
            fig, (lax, ax) = plt.subplots(nrows=2,
                                          gridspec_kw={"height_ratios":[1, 8]})
            
            
            for before, after, name, color in [
                 (self.dist_losses_before_training, 
                      self.dist_losses_after_training,
                      'distribution losses',
                      'green'),
                 (self.eval_losses_before_training, 
                      self.eval_losses_after_training,
                      'evaluation losses',
                      'orange')]:
                assert len(before) == len(after)
                # at 0th step there is no data to evaluate losses
                ax.plot(*zip(*[(a, f[a-1]) for a, f in zip(
                            itertools.chain.from_iterable(zip(
                               range(1, 1 + len(before)),
                               range(1, 1 + len(before)))),
                            itertools.cycle([before, after])
                            )]),
                        'tab:{}'.format(color),
                        label=name)
                ax.scatter(list(range(1, 1 + len(before))), before)
            
            for f, color, label in zip(
                [self.random_win_rates,
                 self.dist_evaluation_random_win_rates,
                 self.eval_evaluation_random_win_rates,
                 # self.uct100_vs_random_win_rates,
                 self.pool_length_record,
                 ],
                ['red', 'purple', 'blue', 'black',
                 'pink', 'grey'],
                ['agent_vs_random',
                 'dist_vs_random',
                 'eval_vs_random',
                 #'uct100_vs_random',
                 'data_fulness',
                 ]):
                    ax.plot(range(len(f)), f,
                            color=color, label=label)

            
            h, l = ax.get_legend_handles_labels()
            lax.legend(h, l, borderaxespad=0)
            lax.axis("off")
            
            plt.tight_layout()
            
            plt.show()
            
        return
    
    def get_losses(self):
        if not hasattr(self, 'x_pool'):
            raise RuntimeError('''There is nothing in the pool to measure
                               losses with.''')
                               
        if self.net.training:
            raise ValueError('''The network cannot be evaluation in training
                             mode.''')
                             
        # It is better to measure the losses when the network is in
        # the training device, but we leave that to the user.
        
        with torch.no_grad():
            optimum_dist_loss = self.d_loss(self.dist_pool, self.dist_pool)
            
            net_dist, net_eval = self.net(self.x_pool)
            
            dist_loss = (self.d_loss(net_dist, self.dist_pool)
                         - optimum_dist_loss)
            eval_loss = self.e_loss(net_eval, self.eval_pool)

        return (dist_loss.item(), eval_loss.item())
    
    # TODO: Add more methods which support the development, and let
    # us do the training parts seperately

    def generate_games(self, num_of_states, ret='raw', append_to_pool=True,
                       append_raw_data = False):
        
        self.net.eval()
        self.net.to(device=self.mpdev)
        
        raw_data = generate_data(
            self.game,
            's',
            num_of_states,
            get_evaluator(self.net, self.mpdev),
            self.mcts_step_self_play,
            self.move_selector_self_play,
            self.temperature_self_play,
            dirichlet_noise_parameter=self.dirichlet_noise_parameter,
            noise_contribution=self.noise_contribution,
            shuffle=True)
        
        x, dist, eva = self.net.translator.translate_data(
            raw_data, self.trdev)
        
        if append_to_pool:
            if hasattr(self, 'x_pool'):
                # add the data and clip if there is excess
                
                excess_data_length = (x.size(dim=0)
                                      + self.x_pool.size(dim = 0)
                                      - self.max_data_length)
                
                if excess_data_length > 0:
                    self.x_pool = self.x_pool[excess_data_length:]
                    self.dist_pool = self.dist_pool[excess_data_length:]
                    self.eval_pool = self.eval_pool[excess_data_length:]
                    if append_raw_data:
                        self.raw_pool = self.raw_pool[excess_data_length:]
                
                self.x_pool = torch.cat((self.x_pool, x), dim = 0)
                self.dist_pool = torch.cat((self.dist_pool, dist), dim = 0)
                self.eval_pool = torch.cat((self.eval_pool, eva), dim = 0)
                if append_raw_data:
                    self.raw_pool.extend(raw_data)
            
            else:
                if x.shape[0] > self.max_data_length:
                    raise ValueError('''max data length is smaller than
                                     the initial pool
                                     ''')
                self.x_pool = x
                self.dist_pool = dist
                self.eval_pool = eva
                if append_raw_data:
                    self.raw_pool.extend(raw_data)
        
        if ret == 'raw':
            return raw_data
        elif ret == 'tensor':
            return (x, dist, eva)
        elif ret == 'None' or ret == 'none' or ret is None:
            return
        else:
            raise ValueError('return type, {}, is not recognized'.format(ret))
        
    
    def train_data(self, graph=True):
        train_data(self.net, self.x_pool, self.dist_pool, self.eval_pool,
                   self.lr, self.momentum, self.bs, self.batches, self.trdev,
                   self.d_loss, self.e_loss, graph=graph)


def train_batches(self, net, data_x, data_dist, data_eval,
                  optimizer, bs, batches, d_loss, e_loss, device):
    
    for parameter in net.parameters():
        if parameter.device != device:
            raise ValueError('The network is not in the device specified.')
    
    if not net.training:
        raise ValueError('Network is not in the training mode.')
    
    if not (data_x.device == data_dist.device == data_eval.device == device):
        raise ValueError('Data device mismatch.')
        
    if not (data_x.shape[0] == data_dist.shape[0] == data_eval.shape[0]):
        raise ValueError('Shape mismatch.')
    
    for _ in range(batches):
        #TODO: make sure this is not a bottleneck when using GPU
        batch_indices = torch.tensor(
            random.sample(range(data_x.shape[0]), bs),
            device=device)
        batch = [torch.index_select(tensor, 0, batch_indices)
                  for tensor in (data_x, data_dist, data_eval)]
        
        batch_x, batch_dist, batch_eval = batch
        
        net_dist, net_eval = net(batch_x)
        
        dist_loss = d_loss(net_dist, batch_dist)
        eval_loss = e_loss(net_eval, batch_eval)
        
        loss = dist_loss + eval_loss
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    
    

def train_data(net,
               data_x,
               data_dist,
               data_eval,
               lr,
               momentum,
               bs,
               batches,
               device,
               d_loss,
               e_loss,
               weight_decay = None,
               graph = True,
               record_frequency = -1):
    """
    

    Parameters
    ----------
    record_frequency : TYPE, optional
        If -1 or not given then the default is 1 per 100.

    Raises
    ------
    ValueError
        DESCRIPTION.

    Returns
    -------
    None.

    """
    
    if weight_decay != None:
        raise NotImplementedError('''Weight decay currently applies to all
                                  parameters, not just weights. Update the
                                  code to use this functionality securely.''')
    else:
        weight_decay = 0.
    
    if record_frequency == -1:
        record_frequency = 100
    
    def record():
        # compute all data and find the training loss
        with torch.no_grad():
            net_dist, net_eval = net(data_x)
            
            dist_loss = d_loss(net_dist, data_dist) - optimum_dist_loss
            eval_loss = e_loss(net_eval, data_eval)

            dist_loss_tracker.append(dist_loss.item())
            eval_loss_tracker.append(eval_loss.item())
    
    net.train()
    net.to(device=device)
    
    if not (data_x.device == data_dist.device == data_eval.device):
        raise ValueError('Device mismatch.')
        
    if not (data_x.shape[0] == data_dist.shape[0] == data_eval.shape[0]):
        raise ValueError('Shape mismatch.')
    
    optimizer = torch.optim.SGD(net.parameters(),
                                lr=lr,
                                momentum=momentum,
                                weight_decay=weight_decay)
    
    dist_loss_tracker = []
    eval_loss_tracker = []
    # Batch loop
    
    optimum_dist_loss = d_loss(data_dist, data_dist)
    
    # Record at initialization.
    record()

    for j, record_counter in zip(range(batches), itertools.count(1)):
        #TODO: make sure this is not a bottleneck when using GPU
        batch_indices = torch.tensor(
            random.sample(range(data_x.shape[0]), bs),
            device=device)
        batch = [torch.index_select(tensor, 0, batch_indices)
                  for tensor in 
                  (data_x,
                  data_dist,
                  data_eval)]
        
        batch_x, batch_dist, batch_eval = batch
        
        net_dist, net_eval = net(batch_x)
        
        dist_loss = d_loss(net_dist, batch_dist)
        eval_loss = e_loss(net_eval, batch_eval)
        
        loss = dist_loss + eval_loss
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # record at given frequency and at the end
        if (record_counter % record_frequency == 0) or (j == batches - 1):
            record()
            if graph:
                fig, ax = plt.subplots()
                
                for f, word, label in zip(
                    [dist_loss_tracker,
                     eval_loss_tracker,
                     ],
                    ['green','orange','red', 'purple', 'blue', 'black'],
                    ['dist_loss', 'eval_loss']):
                    ax.plot(range(len(f)), f, 'tab:{}'.format(word),
                            label=label)
                plt.legend(loc="lower right")
                plt.show()



#%%

        
load_all_uct_data_TCT()

# Settings

common_settings = {
    
# General Settings

'game': Reversi,
'trdev': 'cpu',#training device
'mpdev': 'cpu',#mtcs device

'loop_count': 10,

# Testing Settings

# MCTS
'mcts_step_testing': 10,
'move_selector_testing': azms(4),
'temperature_testing': 0.05,

'dirichlet_noise_parameter': 0.25,
'noise_contribution': 0.25,

# Regression Settings

#Monitoring
'loss_record_frequency': 100,

#Model Settings
'd_loss': cross_entropy,
'e_loss': mse_loss,


}

varied_settings = {

# Self-Play Settings
'mcts_step_self_play': [64],
'move_selector_self_play': [azms(i) for i in [4]],
'temperature_self_play': [0.8],


# Regression Settings

'max_data_length': [60*20000],
'batches': [2*1000*3],
'bs': [30],
'new_states': [60*1000],
'initial_pool': [60*2000],

'lr': [0.01],
'momentum': [0.9],

}

if 'env' in globals():
    print('Environment already exists! Did not change a thing')
else:
    env = Training_Environment(**{  'net': default_network_for_game(
                    common_settings['game'],
                    common_settings['mpdev']),
                **common_settings,
                **{setting: random.choice(values)
                   for setting, values in varied_settings.items()}})
    
#%%

#Console shortcuts

ev = get_evaluator(env.net, env.mpdev)
mcts_ag = MCTSAgent(ev, env.mcts_step_testing,
                  env.move_selector_testing, env.temperature_testing)
dist_ag = EvaluatorAgent(ev) # play with distribution from the network.
monte = lambda x: mcts(x, ev, env.mcts_step_testing,env.move_selector_testing
                       ,env.temperature_testing, return_type = 'distribution')
user = user_for_game(env.game)


#%%

pprint.pprint(env.__dict__)

# env.training()