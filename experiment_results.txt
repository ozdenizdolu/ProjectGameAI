=======================================================

Experiment
5000 states did not fit even with many epochs. Especially the
evaluation loss suffers. Body of the network is 7 resnets with
300 neurons each. Settings were
common_settings = {

'game': Reversi,
'trdev': 'cpu',#training device
'mpdev': 'cpu',#mtcs device

'loop_count': 5,

'd_loss': cross_entropy,
'e_loss': mse_loss,

'weight_decay': None,

# Testing Settings

'in_mcts_testing': True,
# MCTS
'mcts_step_testing': 10,
'move_selector_testing': azms(1),
'temperature_testing': 0.05,
# Evaluator
'move_dist_weight_testing': 1.,
'evaluation_weight_testing': 0.,

'dirichlet_noise_parameter': 0.3,
'noise_contribution': 0.25

}

varied_settings = {

# Self-Play Settings
'mcts_step_self_play': [256],
'move_selector_self_play': [azms(i) for i in [1]],
'temperature_self_play': [0.5],

# Training Settings
'max_data_length': [50000],
'batches': [2500],
'bs': [32],
'new_states': [5000],

'lr': [0.05],
'momentum': [0.9],

}

=======================================================

env.net
Out[10]: 
ReversiNetwork(
  (body): Sequential()
  (dist_head): Sequential(
    (0): Linear(in_features=192, out_features=300, bias=True)
    (1): ReLU()
    (2): Linear(in_features=300, out_features=200, bias=True)
    (3): ReLU()
    (4): Linear(in_features=200, out_features=100, bias=True)
    (5): ReLU()
    (6): Linear(in_features=100, out_features=65, bias=True)
    (7): Softmax(dim=1)
  )
  (eval_head): Sequential(
    (0): Linear(in_features=192, out_features=300, bias=True)
    (1): ReLU()
    (2): Linear(in_features=300, out_features=200, bias=True)
    (3): ReLU()
    (4): Linear(in_features=200, out_features=100, bias=True)
    (5): ReLU()
    (6): Linear(in_features=100, out_features=1, bias=True)
    (7): Tanh()
  )
)

is able to fit 10k fresh generated examples by itself.
Starts from dist 2.3, eval 1.0 error to 0.2 dist, 0.07 eval error in
few epochs.

=====================================================

'net': ReversiNetwork(
  (body): Sequential(
    (0): Linear(in_features=192, out_features=1000, bias=True)
    (1): ReLU()
    (2): ResidualBlock(
      (layer_1): Linear(in_features=1000, out_features=1000, bias=True)
      (batch_norm_1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu_1): ReLU()
      (layer_2): Linear(in_features=1000, out_features=1000, bias=True)
      (batch_norm_2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu_2): ReLU()
    )
    (3): ResidualBlock(
      (layer_1): Linear(in_features=1000, out_features=1000, bias=True)
      (batch_norm_1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu_1): ReLU()
      (layer_2): Linear(in_features=1000, out_features=1000, bias=True)
      (batch_norm_2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu_2): ReLU()
    )
    (4): ResidualBlock(
      (layer_1): Linear(in_features=1000, out_features=1000, bias=True)
      (batch_norm_1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu_1): ReLU()
      (layer_2): Linear(in_features=1000, out_features=1000, bias=True)
      (batch_norm_2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu_2): ReLU()
    )
  )
  (dist_head): Sequential(
    (0): Linear(in_features=1000, out_features=65, bias=True)
    (1): Softmax(dim=1)
  )
  (eval_head): Sequential(
    (0): Linear(in_features=1000, out_features=1, bias=True)
    (1): Tanh()
  )
),

this one made its evaluations -1 immediately at lr = 0.05 but
started to learn with lr=0.001. 5 epochs and it is already great with
35k length low quality data. Still learns, should let it about 30 epochs.
It fits the data great!

=====================================================


=================================================